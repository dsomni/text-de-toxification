{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text detoxification using Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import random_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SEED = 42\n",
    "torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df)=526410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>nontoxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm not gonna have a child... ...with the same...</td>\n",
       "      <td>I'm not going to breed kids with a genetic dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They're all laughing at us, so we'll kick your...</td>\n",
       "      <td>they're laughing at us. We'll show you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maine was very short on black people back then.</td>\n",
       "      <td>there wasn't much black in Maine then.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Come on, Cal, leave that shit alone.</td>\n",
       "      <td>come on, Cal, put it down.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>That night, Li'l Dice satisfied his thirst to ...</td>\n",
       "      <td>that night, he satisfied his blood lust, and k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               toxic  \\\n",
       "0  I'm not gonna have a child... ...with the same...   \n",
       "1  They're all laughing at us, so we'll kick your...   \n",
       "2    Maine was very short on black people back then.   \n",
       "3               Come on, Cal, leave that shit alone.   \n",
       "4  That night, Li'l Dice satisfied his thirst to ...   \n",
       "\n",
       "                                            nontoxic  \n",
       "0  I'm not going to breed kids with a genetic dis...  \n",
       "1            they're laughing at us. We'll show you.  \n",
       "2             there wasn't much black in Maine then.  \n",
       "3                         come on, Cal, put it down.  \n",
       "4  that night, he satisfied his blood lust, and k...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/raw/dataset.csv\")\n",
    "print(f\"{len(df)=}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "SPECIAL_SYMBOLS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "\n",
    "TOKENIZER = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self._preprocess()\n",
    "        self._create_vocab()\n",
    "\n",
    "    def _preprocess(self):\n",
    "        # Clean columns\n",
    "        self.df[\"toxic\"] = self.df[\"toxic\"].str.lower()\n",
    "        self.df[\"nontoxic\"] = self.df[\"nontoxic\"].str.lower()\n",
    "\n",
    "        # Tokenize sentences\n",
    "        self.toxic = self.df[\"toxic\"].apply(TOKENIZER).to_list()\n",
    "        self.nontoxic = self.df[\"nontoxic\"].apply(TOKENIZER).to_list()\n",
    "\n",
    "        self.data = self.toxic + self.nontoxic\n",
    "\n",
    "    def _create_vocab(self):\n",
    "        # creates vocabulary that is used for encoding\n",
    "        # the sequence of tokens (splitted sentence)\n",
    "\n",
    "        self.vocab = build_vocab_from_iterator(\n",
    "            self.data,\n",
    "            min_freq=1,\n",
    "            specials=SPECIAL_SYMBOLS,\n",
    "            special_first=True,\n",
    "        )\n",
    "        self.vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "    def _get_toxic(self, index: int) -> list:\n",
    "        text = self.toxic[index]\n",
    "        return [BOS_IDX] + self.vocab(text) + [EOS_IDX]\n",
    "\n",
    "    def _get_nontoxic(self, index: int) -> list:\n",
    "        text = self.nontoxic[index]\n",
    "        return [BOS_IDX] + self.vocab(text) + [EOS_IDX]\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[list, list]:\n",
    "        return self._get_toxic(index), self._get_nontoxic(index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DetoxificationDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_dataset)=447449\n",
      "len(val_dataset)=52641\n",
      "len(test_dataset)=26320\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [0.85, 0.1, 0.05], generator=torch.Generator().manual_seed(MANUAL_SEED)\n",
    ")\n",
    "print(f\"{len(train_dataset)=}\")\n",
    "print(f\"{len(val_dataset)=}\")\n",
    "print(f\"{len(test_dataset)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_SIZE = 50\n",
    "\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch: list):\n",
    "    toxic_batch, nontoxic_batch = [], []\n",
    "    for _toxic, _nontoxic in batch:\n",
    "        _toxic_tensor = torch.Tensor(_toxic)\n",
    "        _nontoxic_tensor = torch.Tensor(_nontoxic)\n",
    "\n",
    "        toxic_batch.append(_toxic_tensor[:MAX_SIZE])\n",
    "        nontoxic_batch.append(_nontoxic_tensor[:MAX_SIZE])\n",
    "        # if len(_toxic) > MAX_SIZE:\n",
    "        #     toxic_batch.append(_toxic_tensor[:MAX_SIZE])\n",
    "        #     nontoxic_batch.append(_nontoxic_tensor[:MAX_SIZE])\n",
    "        # else:\n",
    "        #     _padding = torch.Tensor([PAD_IDX] * (MAX_SIZE - len(_toxic)))\n",
    "\n",
    "        #     toxic_batch.append(torch.concat((_toxic_tensor, _padding)))\n",
    "        #     nontoxic_batch.append(torch.concat((_nontoxic_tensor, _padding)))\n",
    "\n",
    "    toxic_batch = pad_sequence(toxic_batch, padding_value=PAD_IDX)\n",
    "    nontoxic_batch = pad_sequence(nontoxic_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    return toxic_batch, nontoxic_batch\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 64])\n",
      "torch.Size([42, 64])\n"
     ]
    }
   ],
   "source": [
    "# just to check that all shapes are correct\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    inp, out = batch\n",
    "    print(inp.shape)\n",
    "    print(out.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = MAX_SIZE):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"pos_embedding\", pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(\n",
    "            token_embedding + self.pos_embedding[: token_embedding.size(0), :]\n",
    "        )\n",
    "\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        num_heads: int,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        trg: Tensor,\n",
    "        src_mask: Tensor,\n",
    "        tgt_mask: Tensor,\n",
    "        src_padding_mask: Tensor,\n",
    "        tgt_padding_mask: Tensor,\n",
    "        memory_key_padding_mask: Tensor,\n",
    "    ):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            None,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            memory_key_padding_mask,\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(\n",
    "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
    "        )\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(\n",
    "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask == 0, float(\"-inf\"))\n",
    "        .masked_fill(mask == 1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(MANUAL_SEED)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(dataset.vocab)\n",
    "TGT_VOCAB_SIZE = len(dataset.vocab)\n",
    "EMB_SIZE = 128\n",
    "NUM_HEADS = 2\n",
    "FFN_HID_DIM = 128\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMB_SIZE,\n",
    "    NUM_HEADS,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM,\n",
    ")\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epoch,\n",
    "):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(\n",
    "        loader,\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: train\",\n",
    "        leave=True,\n",
    "    )\n",
    "    for batch in loop:\n",
    "        toxic, nontoxic = batch\n",
    "\n",
    "        toxic, nontoxic = toxic.long().to(DEVICE), nontoxic.long().to(DEVICE)\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "            toxic, nontoxic\n",
    "        )\n",
    "\n",
    "        # forward pass and loss calculation\n",
    "        outputs = model(\n",
    "            toxic,\n",
    "            nontoxic,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask,\n",
    "        )\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        nontoxic = nontoxic.reshape(-1)\n",
    "        loss = loss_fn(outputs, nontoxic)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        total += nontoxic.size(0)\n",
    "\n",
    "        # optimizer run\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix({\"loss\": train_loss / total})\n",
    "\n",
    "\n",
    "def val_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    loss_fn,\n",
    "    epoch,\n",
    "):\n",
    "    loop = tqdm(\n",
    "        loader,\n",
    "        total=len(loader),\n",
    "        desc=f\"Epoch {epoch}: val\",\n",
    "        leave=True,\n",
    "    )\n",
    "    val_loss = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # evaluation mode\n",
    "        for batch in loop:\n",
    "            total += 1\n",
    "            toxic, nontoxic = batch\n",
    "\n",
    "            toxic, nontoxic = toxic.long().to(DEVICE), nontoxic.long().to(DEVICE)\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "                toxic, nontoxic\n",
    "            )\n",
    "\n",
    "            outputs = model(\n",
    "                toxic,\n",
    "                nontoxic,\n",
    "                src_mask,\n",
    "                tgt_mask,\n",
    "                src_padding_mask,\n",
    "                tgt_padding_mask,\n",
    "                src_padding_mask,\n",
    "            )\n",
    "\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "            nontoxic = nontoxic.reshape(-1)\n",
    "\n",
    "            loss = loss_fn(outputs, nontoxic)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            loop.set_postfix({\"loss\": val_loss / total})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: val:   0%|          | 3/823 [00:06<29:18,  2.14s/it, loss=11.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Inno\\PMLDL\\Assignments\\text-detoxification\\notebooks\\2.0-torchtext-transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m start_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m val_one_epoch(model, val_dataloader, loss_fn, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch time = \u001b[39m\u001b[39m{\u001b[39;00m(end_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Inno\\PMLDL\\Assignments\\text-detoxification\\notebooks\\2.0-torchtext-transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m toxic, nontoxic \u001b[39m=\u001b[39m toxic\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(DEVICE), nontoxic\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m src_mask, tgt_mask, src_padding_mask, tgt_padding_mask \u001b[39m=\u001b[39m create_mask(toxic, nontoxic)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(toxic,nontoxic, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, outputs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m nontoxic \u001b[39m=\u001b[39m nontoxic\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Code\\venvs\\text-detoxification-hSirkgjj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Code\\venvs\\text-detoxification-hSirkgjj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32md:\\Inno\\PMLDL\\Assignments\\text-detoxification\\notebooks\\2.0-torchtext-transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m tgt_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtgt_tok_emb(trg))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(src_emb, tgt_emb, src_mask, tgt_mask, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m                         src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Inno/PMLDL/Assignments/text-detoxification/notebooks/2.0-torchtext-transformer.ipynb#X35sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator(outs)\n",
      "File \u001b[1;32md:\\Code\\venvs\\text-detoxification-hSirkgjj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Code\\venvs\\text-detoxification-hSirkgjj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Code\\venvs\\text-detoxification-hSirkgjj\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = timer()\n",
    "    # train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch)\n",
    "    val_one_epoch(model, val_dataloader, loss_fn, epoch)\n",
    "    end_time = timer()\n",
    "    print(f\"Epoch time = {(end_time - start_time):.3f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-detoxification-hSirkgjj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
